{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc40556b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ritvikkhandelwal/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/ritvikkhandelwal/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/ritvikkhandelwal/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.Model_Parameter import get_model_response\n",
    "\n",
    "import re\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "from typing import List\n",
    "import yaml\n",
    "import openai\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from langchain import PromptTemplate\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "import nltk\n",
    "from nltk.translate import meteor_score\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')  # Download the 'punkt' resource\n",
    "nltk.download('wordnet')\n",
    "import nltk\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79d2d13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_parquet('falcon7b_pubmed_summarization.parquet')\n",
    "articles = pd.read_parquet('articles.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09716955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df = pd.merge(data, articles, on='record_id')\n",
    "correct_summary = merged_df['mapped_ground_true'].tolist()\n",
    "model_generated = merged_df['mapped_answer'].tolist()\n",
    "articles = merged_df['article'].tolist()\n",
    "print(len(correct_summary))\n",
    "len(model_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "122d0540",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb208ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize osmosis transcripts and evaluate\n",
    "import time\n",
    "\n",
    "# Start time\n",
    "start_time = time.time()\n",
    "\n",
    "#looping to go accross the entire data for evaluation\n",
    "\n",
    "# Initialize BLEU and ROUGE scorers\n",
    "bleu_scores = []\n",
    "rouge1_scores = []\n",
    "rouge2_scores = []\n",
    "rougeL_scores = []\n",
    "meteor_scores = []\n",
    "\n",
    "rouge_scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'])\n",
    "for i, note in enumerate(correct_summary):\n",
    "    \n",
    "    # Evaluate BLEU score\n",
    "    bleu_score = corpus_bleu([[correct_summary[i].split()]], [model_generated[i].split()])\n",
    "    bleu_scores.append(bleu_score)\n",
    "\n",
    "    # Evaluate ROUGE scores\n",
    "    reference = correct_summary[i]\n",
    "    hypothesis = model_generated[i]\n",
    "    rouge_scores = rouge_scorer.score(reference, hypothesis)\n",
    "\n",
    "    rouge1_scores.append(rouge_scores['rouge1'].fmeasure)\n",
    "    rouge2_scores.append(rouge_scores['rouge2'].fmeasure)\n",
    "    rougeL_scores.append(rouge_scores['rougeL'].fmeasure)\n",
    "\n",
    "     # Tokenize hypothesis and reference summaries\n",
    "    hypothesis_tokens = word_tokenize(hypothesis)\n",
    "    reference_tokens = word_tokenize(reference)\n",
    "\n",
    "    # Evaluate METEOR score\n",
    "    meteor_score_value = meteor_score.meteor_score([reference_tokens], hypothesis_tokens)\n",
    "    meteor_scores.append(meteor_score_value)\n",
    "\n",
    "    #print(\"Clinical Note:\")\n",
    "    #print(note)\n",
    "    print(\"\\nCorrect Summary:\")\n",
    "    print(correct_summary[i])\n",
    "    print(\"\\nGenerated Summary:\")\n",
    "    print(model_generated[i])\n",
    "    print(\"\\nBLEU Score:\", bleu_score)\n",
    "    print(\"ROUGE Scores:\", rouge_scores)\n",
    "    print(\"METEOR Score:\", meteor_score_value)\n",
    "    print(\"--------------------------------------------------\")\n",
    "\n",
    "    \n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db3e7640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BLEU Score: 0.003649077018994482\n",
      "Average ROUGE-1 Score: 0.19479337887518092\n",
      "Average ROUGE-2 Score: 0.03209325966026071\n",
      "Average ROUGE-L Score: 0.12295216808783835\n",
      "Average METEOR Score: 0.11306053015896626\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print average BLEU score\n",
    "avg_bleu_score = sum(bleu_scores) / len(bleu_scores)\n",
    "print(\"Average BLEU Score:\", avg_bleu_score)\n",
    "\n",
    "# Calculate and print average ROUGE scores\n",
    "avg_rouge1_score = sum(rouge1_scores) / len(rouge1_scores)\n",
    "avg_rouge2_score = sum(rouge2_scores) / len(rouge2_scores)\n",
    "avg_rougeL_score = sum(rougeL_scores) / len(rougeL_scores)\n",
    "print(\"Average ROUGE-1 Score:\", avg_rouge1_score)\n",
    "print(\"Average ROUGE-2 Score:\", avg_rouge2_score)\n",
    "print(\"Average ROUGE-L Score:\", avg_rougeL_score)\n",
    "\n",
    "# Calculate and print average METEOR score\n",
    "avg_meteor_score = sum(meteor_scores) / len(meteor_scores)\n",
    "print(\"Average METEOR Score:\", avg_meteor_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2e2d45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n",
      "100\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "print(len(articles))\n",
    "print(len(correct_summary))\n",
    "print(len(model_generated))\n",
    "print(len(meteor_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a612d901",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'articles': articles,\n",
    "    'correct_summary': correct_summary,\n",
    "    'model_generated': model_generated,\n",
    "    'meteor_scores': meteor_scores\n",
    "}\n",
    "\n",
    "# Creating a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Exporting the DataFrame to an Excel file\n",
    "df.to_excel('Falcon_output.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423d693c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
