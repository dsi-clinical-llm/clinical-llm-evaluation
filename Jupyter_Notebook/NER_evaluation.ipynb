{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Saving the DataFrame as an Excel file\n",
        "df_parquet.to_excel(\"falcon_op.xlsx\", index=False)"
      ],
      "metadata": {
        "id": "43M6vf8NH_vE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "Olc4GtS0IB64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CODE FOR EVALAUTION:"
      ],
      "metadata": {
        "id": "XxbNnGUCLjDf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the ground truth and model output Excel files\n",
        "file_path_ground_truth = 'final_NER_answers.xlsx'\n",
        "file_path_model_output = 'final_falcon_answers.xlsx'\n",
        "\n",
        "ground_truth_data = pd.read_excel(file_path_ground_truth)\n",
        "model_output_data = pd.read_excel(file_path_model_output)\n",
        "\n"
      ],
      "metadata": {
        "id": "3AUkXchEDjhL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert 'Entity_name' columns in both datasets to lowercase for case-insensitive comparison\n",
        "ground_truth_data['Entity_name_lower'] = ground_truth_data['Entity_name'].str.lower()\n",
        "model_output_data['Entity_name_lower'] = model_output_data['Entity_name'].str.lower()\n",
        "\n",
        "# Merging the data on 'Abstract Number' and the lowercase 'Entity_name'\n",
        "comparison_data = pd.merge(ground_truth_data, model_output_data,\n",
        "                           left_on=['Abstract Number', 'Entity_name_lower'],\n",
        "                           right_on=['Abstract Number', 'Entity_name_lower'],\n",
        "                           how='outer')\n",
        "\n",
        "# Dropping the lowercase 'Entity_name' column used for merging\n",
        "comparison_data.drop('Entity_name_lower', axis=1, inplace=True)\n",
        "\n",
        "# Filtering out rows where ground truth is missing (keeping rows where model output is missing)\n",
        "comparison_data_filtered = comparison_data.dropna(subset=['Entity_class_x'])\n",
        "\n",
        "# Replacing missing values in the model output column with 'NoPrediction'\n",
        "comparison_data_filtered['Entity_class_y'].fillna('NoPrediction', inplace=True)\n",
        "\n",
        "# Calculating precision, recall, and F1 score\n",
        "precision, recall, f1_score, _ = precision_recall_fscore_support(comparison_data_filtered['Entity_class_x'],\n",
        "                                                                 comparison_data_filtered['Entity_class_y'],\n",
        "                                                                 average='weighted',\n",
        "                                                                 zero_division=0)\n",
        "\n",
        "precision, recall, f1_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PG2Fcof9L1M8",
        "outputId": "6f2b8562-0d1d-48ab-d9ce-6a23705fae28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-61-66411febbd8d>:18: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  comparison_data_filtered['Entity_class_y'].fillna('NoPrediction', inplace=True)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.21851851851851853, 0.004285714285714286, 0.007509253095459993)"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Grouping the filtered data by Abstract Number\n",
        "grouped_data = comparison_data_filtered.groupby('Abstract Number')\n",
        "\n",
        "# Initializing lists to store precision, recall, and F1 score for each abstract\n",
        "precision_list = []\n",
        "recall_list = []\n",
        "f1_score_list = []\n",
        "\n",
        "# Calculating precision, recall, and F1 score for each abstract\n",
        "for _, group in grouped_data:\n",
        "    precision, recall, f1_score, _ = precision_recall_fscore_support(group['Entity_class_x'],\n",
        "                                                                     group['Entity_class_y'],\n",
        "                                                                     average='weighted',\n",
        "                                                                     zero_division=0)\n",
        "    precision_list.append(precision)\n",
        "    recall_list.append(recall)\n",
        "    f1_score_list.append(f1_score)\n",
        "\n",
        "# Averaging the precision, recall, and F1 score across all abstracts\n",
        "average_precision = sum(precision_list) / len(precision_list)\n",
        "average_recall = sum(recall_list) / len(recall_list)\n",
        "average_f1_score = sum(f1_score_list) / len(f1_score_list)\n",
        "\n",
        "average_precision, average_recall, average_f1_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOnpE863MeCo",
        "outputId": "a9c51067-6647-4709-e120-674c8ed734d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.012714285714285713, 0.008428571428571428, 0.009285714285714286)"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tVj9RToROOaQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}